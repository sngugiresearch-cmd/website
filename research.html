<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research - Stanley Ngugi</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg-primary: #2A2A3A;
            --bg-secondary: #35354A;
            --text-primary: #E5E5E5;
            --text-secondary: #A0A0A0;
            --accent: #F78C6C;
            --border: #4A4A5A;
            --hover-bg: #3A3A4A;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'JetBrains Mono', monospace;
            background-color: var(--bg-primary);
            color: var(--text-primary);
            line-height: 1.6;
            font-size: 14px;
        }

        .container {
            max-width: 800px;
            margin: 0 auto;
            padding: 0 20px;
        }

        header {
            padding: 40px 0;
            border-bottom: 1px solid var(--border);
        }

        .header-content {
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .logo {
            font-size: 24px;
            font-weight: 700;
            text-decoration: none;
            color: var(--text-primary);
            border-bottom: 3px solid var(--accent);
            padding-bottom: 2px;
        }

        nav {
            display: flex;
            gap: 30px;
        }

        nav a {
            text-decoration: none;
            color: var(--text-secondary);
            font-weight: 500;
            transition: color 0.2s ease;
            position: relative;
        }

        nav a:hover {
            color: var(--text-primary);
        }

        nav a.active {
            color: var(--text-primary);
        }

        nav a.active::after {
            content: '';
            position: absolute;
            bottom: -4px;
            left: 0;
            right: 0;
            height: 2px;
            background-color: var(--accent);
            border-radius: 1px;
        }

        .controls {
            display: flex;
            gap: 15px;
            align-items: center;
        }

        .icon-btn {
            background: none;
            border: none;
            color: var(--text-secondary);
            cursor: pointer;
            padding: 8px;
            border-radius: 4px;
            transition: all 0.2s ease;
        }

        .icon-btn:hover {
            color: var(--text-primary);
            background-color: var(--hover-bg);
        }

        main {
            padding: 40px 0 120px;
        }

        .page-header {
            margin-bottom: 40px;
        }

        .page-title {
            font-size: 32px;
            font-weight: 700;
            margin-bottom: 16px;
            letter-spacing: -0.02em;
        }

        .research-focus {
            background-color: var(--bg-secondary);
            padding: 30px;
            border-radius: 8px;
            margin-bottom: 60px;
            border-left: 4px solid var(--accent);
        }

        .focus-title {
            font-size: 18px;
            font-weight: 700;
            margin-bottom: 16px;
            color: var(--text-primary);
        }

        .focus-description {
            color: var(--text-secondary);
            line-height: 1.6;
        }

        .focus-description strong {
            color: var(--accent);
            font-weight: 700;
        }

        .section-title {
            font-size: 24px;
            font-weight: 700;
            margin-bottom: 40px;
            padding-bottom: 12px;
            border-bottom: 1px solid var(--border);
        }

        .publications-list {
            display: grid;
            gap: 50px;
        }

        .publication {
            position: relative;
        }

        .pub-title {
            font-size: 18px;
            font-weight: 700;
            color: var(--accent);
            margin-bottom: 12px;
            line-height: 1.3;
        }

        .pub-title a {
            color: inherit;
            text-decoration: none;
            transition: color 0.2s ease;
        }

        .pub-title a:hover {
            color: var(--text-primary);
            text-decoration: underline;
        }

        .pub-meta {
            display: flex;
            flex-wrap: wrap;
            gap: 20px;
            margin-bottom: 16px;
            color: var(--text-secondary);
            font-size: 13px;
        }

        .pub-meta-item {
            display: flex;
            align-items: center;
            gap: 6px;
        }

        .pub-meta-item svg {
            width: 14px;
            height: 14px;
        }

        .abstract-label {
            font-weight: 700;
            color: var(--text-primary);
            margin-bottom: 8px;
            font-size: 14px;
        }

        .pub-abstract {
            color: var(--text-primary);
            line-height: 1.6;
            margin-bottom: 20px;
        }

        .tldr-label {
            font-weight: 700;
            color: var(--text-primary);
            margin-bottom: 8px;
            font-size: 14px;
        }

        .pub-tldr {
            color: var(--text-primary);
            line-height: 1.6;
            margin-bottom: 20px;
        }

        .pub-links {
            display: flex;
            gap: 16px;
            flex-wrap: wrap;
        }

        .pub-link {
            display: inline-flex;
            align-items: center;
            gap: 6px;
            padding: 8px 12px;
            background-color: var(--hover-bg);
            border: 1px solid var(--border);
            border-radius: 4px;
            text-decoration: none;
            color: var(--text-secondary);
            font-size: 13px;
            font-weight: 500;
            transition: all 0.2s ease;
        }

        .pub-link:hover {
            background-color: var(--bg-secondary);
            border-color: var(--accent);
            color: var(--accent);
        }

        .pub-link svg {
            width: 14px;
            height: 14px;
        }

        .research-blog {
            margin-top: 80px;
        }

        @media (max-width: 768px) {
            .header-content {
                flex-direction: column;
                gap: 20px;
            }

            nav {
                gap: 20px;
            }

            .page-title {
                font-size: 28px;
            }

            .container {
                padding: 0 15px;
            }

            .research-focus {
                padding: 20px;
            }

            .pub-meta {
                flex-direction: column;
                gap: 8px;
            }

            .pub-links {
                flex-direction: column;
            }
        }

        @media (max-width: 480px) {
            nav {
                flex-wrap: wrap;
                justify-content: center;
                gap: 15px;
            }

            .page-title {
                font-size: 24px;
            }

            main {
                padding: 60px 0 100px;
            }

            .publications-list {
                gap: 40px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <div class="header-content">
                <a href="/" class="logo">Stanley Ngugi</a>
                
                <nav>
                    <a href="/posts.html">Posts</a>
                    <a href="/research.html" class="active">Research</a>
                    <a href="/about.html">About</a>
                </nav>

                <div class="controls">
                    <button class="icon-btn" aria-label="Search">
                        <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                            <circle cx="11" cy="11" r="8"/>
                            <path d="m21 21-4.35-4.35"/>
                        </svg>
                    </button>
                    <button class="icon-btn" aria-label="Toggle theme">
                        <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                            <circle cx="12" cy="12" r="5"/>
                            <path d="M12 1v2M12 21v2M4.22 4.22l1.42 1.42M18.36 18.36l1.42 1.42M1 12h2M21 12h2M4.22 19.78l1.42-1.42M18.36 5.64l1.42-1.42"/>
                        </svg>
                    </button>
                </div>
            </div>
        </header>

        <main>
            <div class="page-header">
                <h1 class="page-title">Research</h1>
            </div>

            <div class="research-focus">
                <h2 class="focus-title">Current Work</h2>
                <p class="focus-description">
                    I'm currently investigating <strong>metacognition circuits</strong> in large language models. My broader research interests include <strong>polysemanticity</strong> and <strong>circuit analysis</strong> in neural networks.
                </p>
            </div>

            <section>
                <h2 class="section-title">Publications</h2>
                
                <div class="publications-list">
                    <article class="publication">
                        <h3 class="pub-title">
                            <a href="https://arxiv.org/html/2508.07075v1">
                                Surgical Knowledge Rewrite in Compact LLMs: An 'Unlearn-then-Learn' Strategy with ((IA)Â³) for Localized Factual Modulation and Catastrophic Forgetting Mitigation
                            </a>
                        </h3>
                        
                        <div class="pub-meta">
                            <div class="pub-meta-item">
                                <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                    <path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"/>
                                    <polyline points="14,2 14,8 20,8"/>
                                    <line x1="16" y1="13" x2="8" y2="13"/>
                                    <line x1="16" y1="17" x2="8" y2="17"/>
                                    <polyline points="10,9 9,9 8,9"/>
                                </svg>
                                arXiv Preprint
                            </div>
                            <div class="pub-meta-item">
                                <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                    <circle cx="12" cy="12" r="3"/>
                                    <path d="M12 1v6m0 6v6"/>
                                    <path d="M17.196 9l-5.196 3l-5.196-3"/>
                                </svg>
                                August 9, 2025
                            </div>
                            <div class="pub-meta-item">
                                <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                    <path d="M16 4h2a2 2 0 0 1 2 2v14a2 2 0 0 1-2 2H6a2 2 0 0 1-2-2V6a2 2 0 0 1 2-2h2"/>
                                    <rect x="8" y="2" width="8" height="4" rx="1" ry="1"/>
                                </svg>
                                Stanley Ngugi
                            </div>
                        </div>

                        <div class="abstract-label">Abstract</div>
                        <p class="pub-abstract">
                            Large Language Models (LLMs) struggle with dynamic knowledge updates, especially when new information conflicts with deeply embedded facts. Such conflicting factual edits often lead to two critical issues: resistance to adopting the new fact and severe catastrophic forgetting of unrelated knowledge. This paper introduces and evaluates a novel "unlearn-then-learn" strategy for precise knowledge editing in LLMs, leveraging the parameter-efficient fine-tuning (PEFT) technique, Infused Adapter by Inhibiting and Amplifying Inner Activations (IA)Â³. Crucially, this two-stage approach is powered by an initial circuit localization phase that identifies and targets the specific internal components responsible for encoding the conflicting fact. Through a rigorous experimental methodology on microsoft/Phi-3-mini-4k-instruct, we demonstrate that this mechanistically informed two-stage approach achieves near-perfect accuracy (98.50%) for the new, modulated fact while simultaneously effectively suppressing the original conflicting fact (96.00% forget rate). Critically, our strategy exhibits unprecedented localization (72.00% F_control accuracy), dramatically mitigating catastrophic forgetting observed in direct fine-tuning approaches (which showed as low as â¼20% F_control accuracy), a direct benefit of our targeted interpretability-guided intervention. Furthermore, qualitative analysis reveals a nuanced mechanism of "soft forgetting," where original knowledge is suppressed from default retrieval but remains latent and conditionally accessible, enhancing model safety and control. These findings represent a significant advancement towards precise, localized, and safe knowledge management in compact LLMs.
                        </p>

                        <div class="pub-links">
                            <a href="https://arxiv.org/abs/2508.07075" class="pub-link">
                                <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                    <path d="M21 15v4a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2v-4"/>
                                    <polyline points="7,10 12,15 17,10"/>
                                    <line x1="12" y1="15" x2="12" y2="3"/>
                                </svg>
                                arXiv PDF
                            </a>
                            <a href="https://arxiv.org/html/2508.07075v1" class="pub-link">
                                <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                    <circle cx="12" cy="12" r="10"/>
                                    <polyline points="12,6 12,12 16,14"/>
                                </svg>
                                HTML Version
                            </a>
                            <a href="https://github.com/stanleyngugi/surgical_knowledge_editing" class="pub-link">
                                <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                    <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"/>
                                </svg>
                                Code
                            </a>
                        </div>
                    </article>

                    <article class="publication">
                        <h3 class="pub-title">
                            <a href="https://arxiv.org/html/2506.15415v1">
                                Targeted Lexical Injection: Unlocking Latent Cross-Lingual Alignment in Lugha-Llama via Early-Layer LoRA Fine-Tuning
                            </a>
                        </h3>
                        
                        <div class="pub-meta">
                            <div class="pub-meta-item">
                                <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                    <path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"/>
                                    <polyline points="14,2 14,8 20,8"/>
                                    <line x1="16" y1="13" x2="8" y2="13"/>
                                    <line x1="16" y1="17" x2="8" y2="17"/>
                                    <polyline points="10,9 9,9 8,9"/>
                                </svg>
                                arXiv Preprint
                            </div>
                            <div class="pub-meta-item">
                                <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                    <circle cx="12" cy="12" r="3"/>
                                    <path d="M12 1v6m0 6v6"/>
                                    <path d="M17.196 9l-5.196 3l-5.196-3"/>
                                </svg>
                                June 6, 2025
                            </div>
                            <div class="pub-meta-item">
                                <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                    <path d="M16 4h2a2 2 0 0 1 2 2v14a2 2 0 0 1-2 2H6a2 2 0 0 1-2-2V6a2 2 0 0 1 2-2h2"/>
                                    <rect x="8" y="2" width="8" height="4" rx="1" ry="1"/>
                                </svg>
                                Stanley Ngugi
                            </div>
                        </div>

                        <div class="abstract-label">Abstract</div>
                        <p class="pub-abstract">
                            Large Language Models (LLMs) have demonstrated remarkable capabilities, yet their performance in low-resource languages (LRLs), such as Swahili, often lags due to data scarcity and underrepresentation in pre-training. A key challenge is achieving robust cross-lingual lexical alignment, crucial for tasks like translation and cross-lingual information retrieval. This paper introduces Targeted Lexical Injection (TLI), a novel and efficient fine-tuning approach. We first demonstrate that Lugha-Llama-8B-wura, a Swahili-centric LLM, exhibits strong, near-perfect lexical alignment for Swahili-English word pairs in its early internal layers (specifically Layer 2, with 0.99998 average cosine similarity based on a pilot study), a capability not fully reflected in its final output representations (baseline 0.32 similarity on our evaluation set). TLI leverages this insight by using Low-Rank Adaptation (LoRA) and a contrastive learning objective to fine-tune the model, specifically targeting embeddings from this empirically identified optimal early layer. Our experiments show that TLI significantly improves the output-level lexical alignment for 623 trained Swahili-English word pairs, increasing average cosine similarity from 0.3211 to 0.4113 (+28.08%, p < 1.33 Ã 10â»Â²â´â°). More importantly, these improvements generalize remarkably well to 63 unseen control word pairs, with similarity increasing from 0.3143 to 0.4033 (+28.32%, p < 7.17 Ã 10â»Â²â·). These findings suggest TLI enhances the model's ability to preserve and propagate its inherent early-layer cross-lingual knowledge, offering a parameter-efficient and effective strategy for improving lexical alignment in LRL-focused LLMs.
                        </p>

                        <div class="pub-links">
                            <a href="https://arxiv.org/abs/2506.15415" class="pub-link">
                                <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                    <path d="M21 15v4a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2v-4"/>
                                    <polyline points="7,10 12,15 17,10"/>
                                    <line x1="12" y1="15" x2="12" y2="3"/>
                                </svg>
                                arXiv PDF
                            </a>
                            <a href="https://arxiv.org/html/2506.15415v1" class="pub-link">
                                <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                    <circle cx="12" cy="12" r="10"/>
                                    <polyline points="12,6 12,12 16,14"/>
                                </svg>
                                HTML Version
                            </a>
                            <a href="https://github.com/stanleyngugi/tli_project" class="pub-link">
                                <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                    <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"/>
                                </svg>
                                Code
                            </a>
                        </div>
                    </article>
                </div>
            </section>

            <section class="research-blog">
                <h2 class="section-title">Research Blog</h2>
                
                <div class="publications-list">
                    <article class="publication">
                        <h3 class="pub-title">
                            <a href="/files/taming_polysemanticity.html">
                                Taming Incidental Polysemanticity in Toy Models: How Network Training Choices Affect Feature Entanglement
                            </a>
                        </h3>
                        
                        <div class="pub-meta">
                            <div class="pub-meta-item">
                                <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                    <path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"/>
                                    <polyline points="14,2 14,8 20,8"/>
                                    <line x1="16" y1="13" x2="8" y2="13"/>
                                    <line x1="16" y1="17" x2="8" y2="17"/>
                                    <polyline points="10,9 9,9 8,9"/>
                                </svg>
                                Technical Blog Post
                            </div>
                            <div class="pub-meta-item">
                                <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                    <circle cx="12" cy="12" r="3"/>
                                    <path d="M12 1v6m0 6v6"/>
                                    <path d="M17.196 9l-5.196 3l-5.196-3"/>
                                </svg>
                                October 19, 2025
                            </div>
                            <div class="pub-meta-item">
                                <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                    <path d="M16 4h2a2 2 0 0 1 2 2v14a2 2 0 0 1-2 2H6a2 2 0 0 1-2-2V6a2 2 0 0 1 2-2h2"/>
                                    <rect x="8" y="2" width="8" height="4" rx="1" ry="1"/>
                                </svg>
                                Stanley Ngugi
                            </div>
                        </div>

                        <div class="tldr-label">TL;DR</div>
                        <p class="pub-tldr">
                            I built a toy model to test whether network training choices (L1 vs L2 regularization, orthogonal initialization, activation functions, noise) affect the polysemanticity of learned representations. Using sparse autoencoders (SAEs) to measure feature entanglement, I found that networks trained with L2 regularization produced 17.9% less polysemantic representations than L1-trained networks (Interference: 7.14 vs 8.70, t=2.8, p<0.01). This is exploratory work in a toy setting with significant limitations, but it provides mechanistic insights into how training dynamicsânot just SAE architectureâshape interpretability.
                        </p>

                        <div class="pub-links">
                            <a href="/files/taming_polysemanticity.html" class="pub-link">
                                <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                    <path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"/>
                                    <polyline points="14,2 14,8 20,8"/>
                                    <line x1="16" y1="13" x2="8" y2="13"/>
                                    <line x1="16" y1="17" x2="8" y2="17"/>
                                    <polyline points="10,9 9,9 8,9"/>
                                </svg>
                                Read Post
                            </a>
                            <a href="https://github.com/stanleyngugi/taming_polysemanticity" class="pub-link">
                                <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                    <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"/>
                                </svg>
                                Code
                            </a>
                        </div>
                    </article>
                </div>
            </section>
        </main>
    </div>

    <script>
        const themeToggle = document.querySelector('.icon-btn[aria-label="Toggle theme"]');
        if (themeToggle) {
            themeToggle.addEventListener('click', () => {
                console.log('Theme toggle clicked');
            });
        }

        const searchBtn = document.querySelector('.icon-btn[aria-label="Search"]');
        if (searchBtn) {
            searchBtn.addEventListener('click', () => {
                console.log('Search clicked');
            });
        }
    </script>
</body>
</html>
